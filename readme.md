
**Датасет**
https://www.kaggle.com/datasets/mirichoi0218/insurance?select=insurance.csv
Датасет был взят с ресурса kaggle и представляет собой набор данных
о лице, приобретающем страховку (пол, возраст, индекс массы тела, наличие детей, вредные привычки, регион) и целевую переменную - стоимость медицинской страховки

**Препроцессинг датасета**
Прежде всего, закодировал категориальные значения (например, пол, курит/не курит, регион). Построив корреляционный хитмап, заметил, что целевая переменная заметно положительно коррелирует с тем фактом, является ли человек курильщиком или нет, и в меньшей степени от его возраста. Сделал скейлинг целевой переменной и обучающей выборки с помощью fit_transform

**Описание решения**
В качестве лосса взял MSE, с L2-регуляризацией. Далее, нашел частные производные для функции L = 1 / N * sum((X*w + b) - y) + L2 по отношению к весам и биасу. Далее, с помощью алгоритма градиентного спуска (лернинг рейт выбрал 0.01) апдейтил веса и биас каждый проход по обучающей выборке и выводил лосс на экран.
Для реализации минибатчевого GD - добавил функцию батчинга (размера 10)
После каждой эпохи делаю шаффл датасета, чтобы улучшить обобщающую способность модели

**Запуск**:
Все реализации выдерживают единый интерфейс, поэтому запускать можно вот так:
```
eda = EDAApplier(DS_PATH) # препроцессинг
reg = BatchGradientDescent(eda.df) # создаем модель (можно сделать инстанс стохастического спуска StohasticGradientDescent вместо)
reg.descent(epochs=500, alpha=0.01) # спускаемся
reg.make_prediction()
```


**Итог**:
1) Стандартный GD:
    - Loss (MSE):  0.2344848674476016
    - RMSE:  0.48423637559316174
    - R2:  0.7786644641378906
Построенная модель объясняет 77% дисперсии
2) Stohastic GD:
    - Loss (MSE):  0.2402193439212632
    - RMSE:  0.4901217643823453
    - R2:  0.7732515629259599
3) Mini-batch GD:
    - Loss (MSE):  0.2325915982379925
    - RMSE:  0.482277511644481
    - R2:  0.7804515634914713
Из hand-made решений - минибатчевый градиентный спуск показал себя наиболее
качественным в текущей задаче, так как объясняет чуть большую долю дисперсии

4) Sklearn Ridge:
    - MSE: 1.847056755476451
    - RMSE: 1.3590646619923759
    - 0.7814112543629618
Оказалось чуть лучше решений, написанных руками
5) K-Fold таблица:
       mse-train  mse-test  rmse-train  rmse-test  r2-train   r2-test
Fold1   1.671701  1.853947    1.292943   1.361597  0.744832  0.726414
Fold2   1.719144  1.712659    1.311161   1.308686  0.748422  0.711432
Fold3   1.740796  1.618906    1.319392   1.272363  0.733039  0.774003
Fold4   1.729165  1.670669    1.314977   1.292543  0.748971  0.709982
Fold5   1.714199  1.716814    1.309274   1.310272  0.732766  0.776177
Mean    1.715001  1.714599    1.309550   1.309092  0.741606  0.739602
STD     0.023489  0.078120    0.008998   0.029602  0.007248  0.029550
Первое, что можно отметить - модель объясняет только 74% на тестовой выборке.
Кажется, что модель сносно обобщает, если судить по близким значениям метрик
среднего (MSE, RMSE) для обучающей и тестовой выборки


**Классы:**

**Абстрактный класс BaseRegression:**
- Инициализация:
  - Принимает `dataset` (pd.DataFrame) с целевой переменной `charges`.
  - Нормализует все признаки (`StandardScaler`) и целевую переменную. Использую fit_transform.
  - Разбивает данные на `_X_train`, `_X_test`, `_y_train`, `_y_test`.
  - Инициализирует веса (`weights`) и смещение (`bias`).
- Методы:
  - `loss_function(y_pred, y_true, weights)` — MSE с L2-регуляризацией.
  - `loss_without_l2(y_pred, y_true)` — MSE без регуляризации.
  - `lsm_weights` (cached_property) — вычисляет веса методом **наименьших квадратов** с L2-регуляризацией.
  - `least_squares_predict()` — делает предсказания с LSM и возвращает значения в исходном масштабе.
  - `make_prediction()` — предсказывает на тесте с текущими весами и выводит MSE, RMSE и R².
  - `descent()` — абстрактный метод для реализации алгоритма градиентного спуска. Реализуется в классах регрессии со стохастическим, батчевым и мини-батчевым GD.

---

### `StohasticGradientDescent` (SGD)
Реализация стохастического градиентного спуска для линейной регрессии.

**Особенности:**
- `_stohastic_loss_weights_gradient(y_true, y_pred, x)` — градиент по весам для одной строки.
- `_stohastic_loss_bias_gradient(y_true, y_pred)` — градиент по биасу.
- `descent(epochs, alpha)` — обновляет веса и смещение по одному примеру за раз (SGD).

---

### `BatchGradientDescent`
Реализация **градиентного спуска по всему батчу**.

**Особенности:**
- `loss_derivative_over_weights(y_true, y_pred, x)` — градиент MSE + L2 по весам.
- `loss_derivative_over_bias(y_true, y_pred)` — градиент по биасу.
- `descent(epochs, alpha)` — обновляет веса и смещение на каждом эпохе по всему обучающему набору.

---

### `MiniBatchGradientDescent`
Наследник `BatchGradientDescent`, реализует **мини-батч градиентный спуск**.

**Особенности:**
- `create_batches(batch_size)` — создает генератор мини-батчей.
- `descent(epochs, alpha)` — обновляет веса и смещение для каждого мини-батча.

---


### `RidgeRegression`
Реализация Ridge-регрессии (L2) с использованием `sklearn`.

**Особенности:**
- Инициализация создаёт объект `Ridge()`.
- `descent()` — обучает модель на `_X_train` с L2-регуляризацией.
- `make_prediction()` — вычисляет MSE, RMSE и R² для train и test и выводит результаты.

---

### `KFoldRegression`
Класс для оценки модели через **K-Fold кросс-валидацию**.

**Особенности:**
- Использует `KFold(n_splits=5)` для разбиения тренировочного набора.
- `descent()` — обучает `Ridge` на каждом фолде и сохраняет метрики:
  - `mse_train`, `mse_test`, `rmse_train`, `rmse_test`, `r2_train`, `r2_test`.
- `make_table()` — создает Pandas DataFrame со строками метрик и столбцами: Fold1…Foldk, Mean, STD.

---
